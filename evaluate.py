#!/usr/bin/env python3
"""
è¯„ä¼°å’Œå¯è§†åŒ–è„šæœ¬
ç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import argparse
import os
from typing import Dict, Tuple
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, pearsonr
import torch
from sklearn.metrics import mean_absolute_error, mean_squared_error

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
sns.set_palette("husl")


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨ - è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡å¹¶ç”Ÿæˆå¯è§†åŒ–"""
    
    def __init__(self, predictions_csv: str, output_dir: str = "evaluation_results"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            predictions_csv: åŒ…å«é¢„æµ‹ç»“æœçš„ CSV æ–‡ä»¶ï¼ˆéœ€åŒ…å« gt å’Œ pred åˆ—ï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        self.df = pd.read_csv(predictions_csv)
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ“‚ åŠ è½½é¢„æµ‹ç»“æœ: {predictions_csv}")
        print(f"ğŸ“Š æ ·æœ¬æ•°: {len(self.df)}")
    
    def compute_metrics(self) -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # Quality æŒ‡æ ‡
        if 'mos_quality' in self.df.columns and 'quality_score' in self.df.columns:
            q_gt = self.df['mos_quality'].values
            q_pred = self.df['quality_score'].values
            
            # è¿‡æ»¤ NaN å€¼
            valid_mask = ~(np.isnan(q_gt) | np.isnan(q_pred))
            q_gt = q_gt[valid_mask]
            q_pred = q_pred[valid_mask]
            
            metrics['quality_srocc'] = spearmanr(q_gt, q_pred).correlation
            metrics['quality_plcc'] = pearsonr(q_gt, q_pred)[0]
            metrics['quality_mae'] = mean_absolute_error(q_gt, q_pred)
            metrics['quality_rmse'] = np.sqrt(mean_squared_error(q_gt, q_pred))
        
        # Consistency æŒ‡æ ‡
        if 'mos_align' in self.df.columns and 'consistency_score' in self.df.columns:
            c_gt = self.df['mos_align'].values
            c_pred = self.df['consistency_score'].values
            
            # è¿‡æ»¤ NaN å€¼
            valid_mask = ~(np.isnan(c_gt) | np.isnan(c_pred))
            c_gt = c_gt[valid_mask]
            c_pred = c_pred[valid_mask]
            
            metrics['consistency_srocc'] = spearmanr(c_gt, c_pred).correlation
            metrics['consistency_plcc'] = pearsonr(c_gt, c_pred)[0]
            metrics['consistency_mae'] = mean_absolute_error(c_gt, c_pred)
            metrics['consistency_rmse'] = np.sqrt(mean_squared_error(c_gt, c_pred))
        
        return metrics
    
    def plot_scatter(self, save_path: str = None):
        """ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Quality æ•£ç‚¹å›¾
        if 'mos_quality' in self.df.columns and 'quality_score' in self.df.columns:
            ax = axes[0]
            q_gt = self.df['mos_quality']
            q_pred = self.df['quality_score']
            
            ax.scatter(q_gt, q_pred, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
            ax.plot([1, 5], [1, 5], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºæŒ‡æ ‡
            srocc = spearmanr(q_gt, q_pred).correlation
            plcc = pearsonr(q_gt, q_pred)[0]
            mae = mean_absolute_error(q_gt, q_pred)
            
            ax.set_xlabel('çœŸå®è´¨é‡åˆ†æ•° (Ground Truth)', fontsize=12)
            ax.set_ylabel('é¢„æµ‹è´¨é‡åˆ†æ•° (Predicted)', fontsize=12)
            ax.set_title(f'Quality é¢„æµ‹\nSROCC={srocc:.4f}, PLCC={plcc:.4f}, MAE={mae:.3f}',
                        fontsize=13, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_xlim(0.5, 5.5)
            ax.set_ylim(0.5, 5.5)
        
        # Consistency æ•£ç‚¹å›¾
        if 'mos_align' in self.df.columns and 'consistency_score' in self.df.columns:
            ax = axes[1]
            c_gt = self.df['mos_align']
            c_pred = self.df['consistency_score']
            
            ax.scatter(c_gt, c_pred, alpha=0.5, s=30, edgecolors='k', linewidth=0.5,
                      color='orange')
            ax.plot([1, 5], [1, 5], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹')
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºæŒ‡æ ‡
            srocc = spearmanr(c_gt, c_pred).correlation
            plcc = pearsonr(c_gt, c_pred)[0]
            mae = mean_absolute_error(c_gt, c_pred)
            
            ax.set_xlabel('çœŸå®ä¸€è‡´æ€§åˆ†æ•° (Ground Truth)', fontsize=12)
            ax.set_ylabel('é¢„æµ‹ä¸€è‡´æ€§åˆ†æ•° (Predicted)', fontsize=12)
            ax.set_title(f'Consistency é¢„æµ‹\nSROCC={srocc:.4f}, PLCC={plcc:.4f}, MAE={mae:.3f}',
                        fontsize=13, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_xlim(0.5, 5.5)
            ax.set_ylim(0.5, 5.5)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
        else:
            plt.savefig(os.path.join(self.output_dir, 'scatter_plot.png'),
                       dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_error_distribution(self, save_path: str = None):
        """ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # Quality è¯¯å·®åˆ†å¸ƒ
        if 'mos_quality' in self.df.columns and 'quality_score' in self.df.columns:
            q_error = self.df['quality_score'] - self.df['mos_quality']
            
            # ç›´æ–¹å›¾
            ax = axes[0, 0]
            ax.hist(q_error, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            ax.axvline(0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®')
            ax.axvline(q_error.mean(), color='green', linestyle='--', linewidth=2,
                      label=f'å¹³å‡è¯¯å·® ({q_error.mean():.3f})')
            ax.set_xlabel('é¢„æµ‹è¯¯å·® (Predicted - Ground Truth)', fontsize=11)
            ax.set_ylabel('é¢‘æ•°', fontsize=11)
            ax.set_title(f'Quality è¯¯å·®åˆ†å¸ƒ\nStd={q_error.std():.3f}',
                        fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # ç®±çº¿å›¾
            ax = axes[0, 1]
            ax.boxplot([q_error], vert=True, labels=['Quality'], patch_artist=True,
                      boxprops=dict(facecolor='skyblue'))
            ax.axhline(0, color='red', linestyle='--', linewidth=2)
            ax.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=11)
            ax.set_title('Quality è¯¯å·®ç®±çº¿å›¾', fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        # Consistency è¯¯å·®åˆ†å¸ƒ
        if 'mos_align' in self.df.columns and 'consistency_score' in self.df.columns:
            c_error = self.df['consistency_score'] - self.df['mos_align']
            
            # ç›´æ–¹å›¾
            ax = axes[1, 0]
            ax.hist(c_error, bins=50, alpha=0.7, color='orange', edgecolor='black')
            ax.axvline(0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®')
            ax.axvline(c_error.mean(), color='green', linestyle='--', linewidth=2,
                      label=f'å¹³å‡è¯¯å·® ({c_error.mean():.3f})')
            ax.set_xlabel('é¢„æµ‹è¯¯å·® (Predicted - Ground Truth)', fontsize=11)
            ax.set_ylabel('é¢‘æ•°', fontsize=11)
            ax.set_title(f'Consistency è¯¯å·®åˆ†å¸ƒ\nStd={c_error.std():.3f}',
                        fontsize=12, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # ç®±çº¿å›¾
            ax = axes[1, 1]
            ax.boxplot([c_error], vert=True, labels=['Consistency'], patch_artist=True,
                      boxprops=dict(facecolor='orange'))
            ax.axhline(0, color='red', linestyle='--', linewidth=2)
            ax.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=11)
            ax.set_title('Consistency è¯¯å·®ç®±çº¿å›¾', fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ è¯¯å·®åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
        else:
            plt.savefig(os.path.join(self.output_dir, 'error_distribution.png'),
                       dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_score_distribution(self, save_path: str = None):
        """ç»˜åˆ¶åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Quality åˆ†æ•°åˆ†å¸ƒ
        if 'mos_quality' in self.df.columns and 'quality_score' in self.df.columns:
            ax = axes[0]
            ax.hist(self.df['mos_quality'], bins=30, alpha=0.6, label='çœŸå®å€¼',
                   color='blue', edgecolor='black')
            ax.hist(self.df['quality_score'], bins=30, alpha=0.6, label='é¢„æµ‹å€¼',
                   color='red', edgecolor='black')
            ax.set_xlabel('Quality åˆ†æ•°', fontsize=12)
            ax.set_ylabel('é¢‘æ•°', fontsize=12)
            ax.set_title('Quality åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”', fontsize=13, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # Consistency åˆ†æ•°åˆ†å¸ƒ
        if 'mos_align' in self.df.columns and 'consistency_score' in self.df.columns:
            ax = axes[1]
            ax.hist(self.df['mos_align'], bins=30, alpha=0.6, label='çœŸå®å€¼',
                   color='blue', edgecolor='black')
            ax.hist(self.df['consistency_score'], bins=30, alpha=0.6, label='é¢„æµ‹å€¼',
                   color='red', edgecolor='black')
            ax.set_xlabel('Consistency åˆ†æ•°', fontsize=12)
            ax.set_ylabel('é¢‘æ•°', fontsize=12)
            ax.set_title('Consistency åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”', fontsize=13, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ åˆ†æ•°åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
        else:
            plt.savefig(os.path.join(self.output_dir, 'score_distribution.png'),
                       dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def generate_report(self, save_path: str = None):
        """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šï¼ˆæ–‡æœ¬ï¼‰"""
        metrics = self.compute_metrics()
        
        report_lines = [
            "=" * 80,
            "ğŸ“Š æ¨¡å‹è¯„ä¼°æŠ¥å‘Š",
            "=" * 80,
            "",
            f"ğŸ“‚ æ•°æ®é›†: {len(self.df)} ä¸ªæ ·æœ¬",
            "",
            "ğŸ¯ Quality è¯„ä¼°æŒ‡æ ‡:",
            "-" * 80,
        ]
        
        if 'quality_srocc' in metrics:
            report_lines.extend([
                f"  â€¢ SROCC (Spearman):    {metrics['quality_srocc']:.4f}",
                f"  â€¢ PLCC (Pearson):      {metrics['quality_plcc']:.4f}",
                f"  â€¢ MAE:                 {metrics['quality_mae']:.4f}",
                f"  â€¢ RMSE:                {metrics['quality_rmse']:.4f}",
            ])
        
        report_lines.extend([
            "",
            "ğŸ”— Consistency è¯„ä¼°æŒ‡æ ‡:",
            "-" * 80,
        ])
        
        if 'consistency_srocc' in metrics:
            report_lines.extend([
                f"  â€¢ SROCC (Spearman):    {metrics['consistency_srocc']:.4f}",
                f"  â€¢ PLCC (Pearson):      {metrics['consistency_plcc']:.4f}",
                f"  â€¢ MAE:                 {metrics['consistency_mae']:.4f}",
                f"  â€¢ RMSE:                {metrics['consistency_rmse']:.4f}",
            ])
        
        report_lines.extend([
            "",
            "=" * 80,
            "ğŸ“ˆ æ€§èƒ½ç­‰çº§è¯„ä¼°:",
            "-" * 80,
        ])
        
        # æ€§èƒ½è¯„çº§
        if 'quality_srocc' in metrics:
            q_srocc = metrics['quality_srocc']
            q_grade = "ä¼˜ç§€" if q_srocc > 0.90 else "è‰¯å¥½" if q_srocc > 0.85 else "åŠæ ¼" if q_srocc > 0.80 else "è¾ƒå·®"
            report_lines.append(f"  â€¢ Quality SROCC:       {q_grade} ({q_srocc:.4f})")
        
        if 'consistency_srocc' in metrics:
            c_srocc = metrics['consistency_srocc']
            c_grade = "ä¼˜ç§€" if c_srocc > 0.90 else "è‰¯å¥½" if c_srocc > 0.85 else "åŠæ ¼" if c_srocc > 0.80 else "è¾ƒå·®"
            report_lines.append(f"  â€¢ Consistency SROCC:   {c_grade} ({c_srocc:.4f})")
        
        report_lines.extend([
            "",
            "=" * 80,
            "ğŸ’¡ å»ºè®®:",
            "-" * 80,
        ])
        
        # ç»™å‡ºæ”¹è¿›å»ºè®®
        if 'quality_srocc' in metrics and metrics['quality_srocc'] < 0.85:
            report_lines.append("  âš ï¸  Quality æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®ï¼š")
            report_lines.append("     - å¢å¤§ --residual_scale_q å‚æ•°")
            report_lines.append("     - å¢å¤§ --w_q æŸå¤±æƒé‡")
            report_lines.append("     - å‡å°‘å†»ç»“å±‚æ•°ï¼ˆæ›´å¤šå±‚å¯è®­ç»ƒï¼‰")
        
        if 'consistency_srocc' in metrics and metrics['consistency_srocc'] < 0.85:
            report_lines.append("  âš ï¸  Consistency æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®ï¼š")
            report_lines.append("     - å¢å¤§ --residual_scale_c å‚æ•°")
            report_lines.append("     - å¢å¤§ --w_c æŸå¤±æƒé‡")
            report_lines.append("     - è€ƒè™‘ä½¿ç”¨ --use_refinement é€‰é¡¹")
        
        if 'quality_srocc' in metrics and metrics['quality_srocc'] >= 0.90 and \
           'consistency_srocc' in metrics and metrics['consistency_srocc'] >= 0.90:
            report_lines.append("  âœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼å¯ä»¥è€ƒè™‘ï¼š")
            report_lines.append("     - åœ¨æ›´å¤§æ•°æ®é›†ä¸Šæµ‹è¯•æ³›åŒ–èƒ½åŠ›")
            report_lines.append("     - å¯¼å‡ºä¸º ONNX ç”¨äºéƒ¨ç½²")
            report_lines.append("     - å°è¯•æ¨¡å‹è’¸é¦ä»¥å‡å°æ¨¡å‹å¤§å°")
        
        report_lines.append("=" * 80)
        
        report = "\n".join(report_lines)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print(report)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if save_path:
            report_path = save_path
        else:
            report_path = os.path.join(self.output_dir, 'evaluation_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ’¾ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return metrics
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print("\n" + "=" * 80)
        print("ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°...")
        print("=" * 80 + "\n")
        
        # 1. è®¡ç®—æŒ‡æ ‡
        print("1ï¸âƒ£  è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = self.compute_metrics()
        
        # 2. ç”Ÿæˆæ•£ç‚¹å›¾
        print("2ï¸âƒ£  ç”Ÿæˆæ•£ç‚¹å›¾...")
        self.plot_scatter()
        
        # 3. ç”Ÿæˆè¯¯å·®åˆ†å¸ƒå›¾
        print("3ï¸âƒ£  ç”Ÿæˆè¯¯å·®åˆ†å¸ƒå›¾...")
        self.plot_error_distribution()
        
        # 4. ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾
        print("4ï¸âƒ£  ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾...")
        self.plot_score_distribution()
        
        # 5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        print("5ï¸âƒ£  ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...\n")
        self.generate_report()
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°:", self.output_dir)
        
        return metrics


def main():
    parser = argparse.ArgumentParser(
        description="æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–å·¥å…·",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--predictions_csv', type=str, required=True,
                       help='åŒ…å«é¢„æµ‹ç»“æœçš„ CSV æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--plot_only', action='store_true',
                       help='ä»…ç”Ÿæˆå›¾è¡¨ï¼Œä¸è®¡ç®—æŒ‡æ ‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(args.predictions_csv, args.output_dir)
    
    if args.plot_only:
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        evaluator.plot_scatter()
        evaluator.plot_error_distribution()
        evaluator.plot_score_distribution()
        print("âœ… å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
    else:
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        evaluator.run_full_evaluation()


if __name__ == "__main__":
    main()
