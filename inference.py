#!/usr/bin/env python3
"""
æ¨ç†è„šæœ¬ - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œè´¨é‡å’Œä¸€è‡´æ€§è¯„ä¼°
æ”¯æŒå•å¼ å›¾åƒå’Œæ‰¹é‡å›¾åƒè¯„ä¼°
"""

import argparse
import os
from typing import List, Tuple, Optional
import torch
import torch.nn.functional as F
from PIL import Image
from transformers import CLIPProcessor
from torchvision import transforms
import pandas as pd
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹å®šä¹‰
from baseline import BaselineCLIPScore


class ImageQualityPredictor:
    """å›¾åƒè´¨é‡å’Œä¸€è‡´æ€§é¢„æµ‹å™¨"""
    
    def __init__(
        self,
        model_path: str,
        clip_model_name: str = "openai/clip-vit-large-patch14",
        device: str = "cuda",
        use_residual_learning: bool = True,
        partial_freeze: bool = False,
        freeze_layers: int = 18,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è·¯å¾„
            clip_model_name: CLIP æ¨¡å‹åç§°
            device: è®¾å¤‡ (cuda/cpu)
            use_residual_learning: æ˜¯å¦ä½¿ç”¨æ®‹å·®å­¦ä¹ 
            partial_freeze: æ˜¯å¦éƒ¨åˆ†å†»ç»“
            freeze_layers: å†»ç»“å±‚æ•°
            dropout: Dropout æ¯”ä¾‹
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½ CLIP å¤„ç†å™¨
        print(f"ğŸ“¦ åŠ è½½ CLIP å¤„ç†å™¨: {clip_model_name}")
        self.processor = CLIPProcessor.from_pretrained(clip_model_name)
        
        # åˆ›å»ºå›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                self.processor.image_processor.image_mean,
                self.processor.image_processor.image_std
            )
        ])
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
        self.model = BaselineCLIPScore(
            clip_model_name=clip_model_name,
            freeze=False,
            use_residual_learning=use_residual_learning,
            residual_scale_q=0.2,
            residual_scale_c=0.2,
            partial_freeze=partial_freeze,
            freeze_layers=freeze_layers,
            dropout=dropout
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        state_dict = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    @torch.no_grad()
    def predict_single(
        self,
        image_path: str,
        prompt: str,
        return_details: bool = False
    ) -> Tuple[float, float]:
        """
        é¢„æµ‹å•å¼ å›¾åƒçš„è´¨é‡å’Œä¸€è‡´æ€§
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            prompt: æ–‡æœ¬æç¤ºè¯
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ coarse scoreï¼‰
            
        Returns:
            (quality_score, consistency_score) æˆ–
            (quality_score, consistency_score, c_coarse_score) å¦‚æœ return_details=True
        """
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert("RGB")
        
        # é¢„å¤„ç†å›¾åƒ
        pixel_values = self.transform(image).unsqueeze(0).to(self.device)
        
        # å¤„ç†æ–‡æœ¬
        text_inputs = self.processor(
            text=[prompt],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=77
        )
        input_ids = text_inputs.input_ids.to(self.device)
        attention_mask = text_inputs.attention_mask.to(self.device)
        
        # æ¨ç†
        q_pred, c_pred, _, _, c_coarse = self.model(pixel_values, input_ids, attention_mask)
        
        # è½¬æ¢ä¸º 1-5 åˆ†åˆ¶
        quality_score = q_pred.item() * 5.0
        consistency_score = c_pred.item() * 5.0
        
        if return_details:
            c_coarse_score = c_coarse.item() * 5.0
            return quality_score, consistency_score, c_coarse_score
        
        return quality_score, consistency_score
    
    def predict_batch(
        self,
        image_paths: List[str],
        prompts: List[str],
        batch_size: int = 32,
        show_progress: bool = True
    ) -> pd.DataFrame:
        """
        æ‰¹é‡é¢„æµ‹å›¾åƒè´¨é‡å’Œä¸€è‡´æ€§
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            prompts: æ–‡æœ¬æç¤ºè¯åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„ DataFrame
        """
        assert len(image_paths) == len(prompts), "å›¾åƒæ•°é‡å’Œæç¤ºè¯æ•°é‡å¿…é¡»ç›¸åŒ"
        
        results = []
        iterator = range(0, len(image_paths), batch_size)
        
        if show_progress:
            iterator = tqdm(iterator, desc="é¢„æµ‹è¿›åº¦")
        
        for i in iterator:
            batch_images = image_paths[i:i + batch_size]
            batch_prompts = prompts[i:i + batch_size]
            
            # æ‰¹é‡é¢„å¤„ç†å›¾åƒ
            pixel_values_list = []
            for img_path in batch_images:
                try:
                    image = Image.open(img_path).convert("RGB")
                    pixel_values = self.transform(image)
                    pixel_values_list.append(pixel_values)
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½å›¾åƒå¤±è´¥: {img_path}, é”™è¯¯: {e}")
                    continue
            
            if not pixel_values_list:
                continue
            
            pixel_values = torch.stack(pixel_values_list).to(self.device)
            
            # æ‰¹é‡å¤„ç†æ–‡æœ¬
            text_inputs = self.processor(
                text=batch_prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=77
            )
            input_ids = text_inputs.input_ids.to(self.device)
            attention_mask = text_inputs.attention_mask.to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                q_pred, c_pred, _, _, _ = self.model(pixel_values, input_ids, attention_mask)
            
            # æ”¶é›†ç»“æœ
            for j, (img_path, prompt) in enumerate(zip(batch_images, batch_prompts)):
                results.append({
                    'image_path': img_path,
                    'prompt': prompt,
                    'quality_score': q_pred[j].item() * 5.0,
                    'consistency_score': c_pred[j].item() * 5.0
                })
        
        return pd.DataFrame(results)
    
    def predict_from_csv(
        self,
        csv_path: str,
        image_dir: str,
        output_path: Optional[str] = None,
        batch_size: int = 32
    ) -> pd.DataFrame:
        """
        ä» CSV æ–‡ä»¶è¯»å–æ•°æ®å¹¶æ‰¹é‡é¢„æµ‹
        
        Args:
            csv_path: CSV æ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å« 'name' å’Œ 'prompt' åˆ—ï¼‰
            image_dir: å›¾åƒç›®å½•
            output_path: è¾“å‡º CSV è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„ DataFrame
        """
        print(f"ğŸ“‚ è¯»å– CSV æ–‡ä»¶: {csv_path}")
        df = pd.read_csv(csv_path)
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        if 'name' not in df.columns or 'prompt' not in df.columns:
            raise ValueError("CSV æ–‡ä»¶å¿…é¡»åŒ…å« 'name' å’Œ 'prompt' åˆ—")
        
        # æ„å»ºå›¾åƒè·¯å¾„
        image_paths = [os.path.join(image_dir, name) for name in df['name']]
        prompts = df['prompt'].tolist()
        
        # æ‰¹é‡é¢„æµ‹
        print(f"ğŸš€ å¼€å§‹é¢„æµ‹ {len(image_paths)} å¼ å›¾åƒ...")
        results_df = self.predict_batch(image_paths, prompts, batch_size=batch_size)
        
        # åˆå¹¶åŸå§‹æ•°æ®å’Œé¢„æµ‹ç»“æœ
        results_df = pd.merge(
            df,
            results_df[['image_path', 'quality_score', 'consistency_score']],
            left_on=df['name'].apply(lambda x: os.path.join(image_dir, x)),
            right_on='image_path',
            how='left'
        )
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—è¯¯å·®
        if 'mos_quality' in df.columns:
            results_df['quality_error'] = abs(results_df['quality_score'] - results_df['mos_quality'])
        if 'mos_align' in df.columns:
            results_df['consistency_error'] = abs(results_df['consistency_score'] - results_df['mos_align'])
        
        # ä¿å­˜ç»“æœ
        if output_path:
            results_df.to_csv(output_path, index=False)
            print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results_df


def main():
    parser = argparse.ArgumentParser(
        description="AIGC å›¾åƒè´¨é‡å’Œä¸€è‡´æ€§è¯„ä¼° - æ¨ç†è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model_path', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--clip_model_name', type=str,
                        default='openai/clip-vit-large-patch14',
                        help='CLIP æ¨¡å‹åç§°æˆ–è·¯å¾„')
    
    # æ¨ç†æ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--single', nargs=2, metavar=('IMAGE', 'PROMPT'),
                           help='å•å¼ å›¾åƒæ¨ç†ï¼š--single image.jpg "a cat"')
    mode_group.add_argument('--batch', nargs=2, metavar=('IMAGE_DIR', 'PROMPTS_FILE'),
                           help='æ‰¹é‡æ¨ç†ï¼š--batch images/ prompts.txt')
    mode_group.add_argument('--csv', nargs=2, metavar=('CSV_FILE', 'IMAGE_DIR'),
                           help='ä» CSV æ¨ç†ï¼š--csv data.csv images/')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--use_residual_learning', action='store_true', default=True,
                        help='ä½¿ç”¨æ®‹å·®å­¦ä¹ æ¨¡å¼')
    parser.add_argument('--partial_freeze', action='store_true',
                        help='éƒ¨åˆ†å†»ç»“æ¨¡å¼')
    parser.add_argument('--freeze_layers', type=int, default=18,
                        help='å†»ç»“å±‚æ•°')
    
    # æ¨ç†è®¾ç½®
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹é‡æ¨ç†çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', type=str, default='cuda',
                        choices=['cuda', 'cpu'],
                        help='æ¨ç†è®¾å¤‡')
    parser.add_argument('--output', type=str,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ‰¹é‡æ¨ç†æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„æµ‹å™¨
    print("=" * 80)
    print("ğŸ¯ AIGC å›¾åƒè´¨é‡ä¸ä¸€è‡´æ€§è¯„ä¼° - æ¨ç†æ¨¡å¼")
    print("=" * 80 + "\n")
    
    predictor = ImageQualityPredictor(
        model_path=args.model_path,
        clip_model_name=args.clip_model_name,
        device=args.device,
        use_residual_learning=args.use_residual_learning,
        partial_freeze=args.partial_freeze,
        freeze_layers=args.freeze_layers
    )
    
    # å•å¼ å›¾åƒæ¨ç†
    if args.single:
        image_path, prompt = args.single
        print(f"ğŸ–¼ï¸  å›¾åƒ: {image_path}")
        print(f"ğŸ“ æç¤ºè¯: {prompt}\n")
        
        quality, consistency, coarse = predictor.predict_single(
            image_path, prompt, return_details=True
        )
        
        print("=" * 80)
        print("ğŸ“Š é¢„æµ‹ç»“æœ:")
        print("-" * 80)
        print(f"  ğŸ¨ å›¾åƒè´¨é‡ (Quality):         {quality:.2f} / 5.00")
        print(f"  ğŸ”— æ–‡æœ¬ä¸€è‡´æ€§ (Consistency):   {consistency:.2f} / 5.00")
        if args.use_residual_learning:
            print(f"  ğŸ“ˆ åŸºå‡†åˆ†æ•° (Coarse/Base):     {coarse:.2f} / 5.00")
            print(f"  â• æ®‹å·®ä¿®æ­£ (Residual):        {consistency - coarse:+.2f}")
        print("=" * 80)
    
    # æ‰¹é‡æ¨ç†
    elif args.batch:
        image_dir, prompts_file = args.batch
        
        # è¯»å–æç¤ºè¯æ–‡ä»¶
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
        
        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        image_files = sorted([
            os.path.join(image_dir, f)
            for f in os.listdir(image_dir)
            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))
        ])
        
        if len(image_files) != len(prompts):
            print(f"âš ï¸  è­¦å‘Š: å›¾åƒæ•°é‡ ({len(image_files)}) ä¸æç¤ºè¯æ•°é‡ ({len(prompts)}) ä¸åŒ¹é…")
            min_len = min(len(image_files), len(prompts))
            image_files = image_files[:min_len]
            prompts = prompts[:min_len]
        
        # æ‰¹é‡é¢„æµ‹
        results = predictor.predict_batch(
            image_files, prompts, batch_size=args.batch_size
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 80)
        print("ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        print("-" * 80)
        print(f"  æ€»å›¾åƒæ•°: {len(results)}")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°:     {results['quality_score'].mean():.2f} / 5.00")
        print(f"  å¹³å‡ä¸€è‡´æ€§åˆ†æ•°:   {results['consistency_score'].mean():.2f} / 5.00")
        print(f"  è´¨é‡åˆ†æ•°æ ‡å‡†å·®:   {results['quality_score'].std():.2f}")
        print(f"  ä¸€è‡´æ€§åˆ†æ•°æ ‡å‡†å·®: {results['consistency_score'].std():.2f}")
        print("=" * 80)
        
        # ä¿å­˜ç»“æœ
        if args.output:
            results.to_csv(args.output, index=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    
    # ä» CSV æ¨ç†
    elif args.csv:
        csv_file, image_dir = args.csv
        
        results = predictor.predict_from_csv(
            csv_file, image_dir,
            output_path=args.output,
            batch_size=args.batch_size
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 80)
        print("ğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        print("-" * 80)
        print(f"  æ€»å›¾åƒæ•°: {len(results)}")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°:     {results['quality_score'].mean():.2f} / 5.00")
        print(f"  å¹³å‡ä¸€è‡´æ€§åˆ†æ•°:   {results['consistency_score'].mean():.2f} / 5.00")
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œæ˜¾ç¤ºè¯¯å·®
        if 'quality_error' in results.columns:
            print(f"\n  ğŸ“‰ è¯¯å·®åˆ†æ:")
            print(f"    è´¨é‡ MAE:    {results['quality_error'].mean():.3f}")
            print(f"    ä¸€è‡´æ€§ MAE:  {results['consistency_error'].mean():.3f}")
        
        print("=" * 80)
    
    print("\nâœ… æ¨ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
