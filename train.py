#!/usr/bin/env python3
"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬
ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒæ—¥å¿—ã€æ··åˆç²¾åº¦è®­ç»ƒã€æ›´å¥½çš„é”™è¯¯å¤„ç†
"""

import os
import sys
import logging
from datetime import datetime
from typing import Tuple
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from transformers import get_cosine_schedule_with_warmup
from scipy.stats import spearmanr, pearsonr
from tqdm import tqdm
import pandas as pd

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from config import Config, create_parser, parse_args_to_config
from baseline import BaselineCLIPScore, BaselineDataset, collate_fn, CLIPProcessor
from torchvision import transforms


def setup_logging(config: Config) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(config.training.log_dir, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(
        config.training.log_dir,
        f"{config.experiment_name}_{timestamp}.log"
    )
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return logger


def train_epoch(model, dl, opt, sched, config, scaler, logger, epoch):
    """è®­ç»ƒä¸€ä¸ª epochï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰"""
    model.train()
    totals = [0.0, 0.0, 0.0, 0.0]  # total, lq, lc, le
    
    pbar = tqdm(dl, desc=f"Epoch {epoch}")
    for batch_idx, batch in enumerate(pbar):
        # è§£åŒ…æ•°æ®
        px, ids, mask, q_t, c_t, exp_ids, exp_mask = batch
        px = px.to(config.training.device)
        ids = ids.to(config.training.device)
        mask = mask.to(config.training.device)
        q_t = q_t.to(config.training.device)
        c_t = c_t.to(config.training.device)
        
        if exp_ids is not None:
            exp_ids = exp_ids.to(config.training.device)
            exp_mask = exp_mask.to(config.training.device)
        
        opt.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        try:
            if config.training.mixed_precision:
                with autocast():
                    q_p, c_p, img_g, txt_g, c_coarse = model(px, ids, mask)
                    lq = F.mse_loss(q_p, q_t)
                    lc = F.mse_loss(c_p, c_t)
                    loss = config.training.w_q * lq + config.training.w_c * lc
                    
                    le = torch.tensor(0.0, device=config.training.device)
                    if config.model.use_explanations and exp_ids is not None:
                        le = model.compute_rationale_alignment_loss(img_g, txt_g, exp_ids, exp_mask)
                        loss = loss + config.training.w_exp * le
                
                # åå‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if config.training.max_grad_norm > 0:
                    scaler.unscale_(opt)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
                
                scaler.step(opt)
                scaler.update()
            else:
                # å¸¸è§„è®­ç»ƒ
                q_p, c_p, img_g, txt_g, c_coarse = model(px, ids, mask)
                lq = F.mse_loss(q_p, q_t)
                lc = F.mse_loss(c_p, c_t)
                loss = config.training.w_q * lq + config.training.w_c * lc
                
                le = torch.tensor(0.0, device=config.training.device)
                if config.model.use_explanations and exp_ids is not None:
                    le = model.compute_rationale_alignment_loss(img_g, txt_g, exp_ids, exp_mask)
                    loss = loss + config.training.w_exp * le
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if config.training.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.max_grad_norm)
                
                opt.step()
            
            sched.step()
            
            # ç´¯ç§¯æŸå¤±
            totals[0] += loss.item()
            totals[1] += lq.item()
            totals[2] += lc.item()
            totals[3] += le.item() if torch.is_tensor(le) else 0.0
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lq': f'{lq.item():.4f}',
                'lc': f'{lc.item():.4f}'
            })
            
            # å®šæœŸè®°å½•æ—¥å¿—
            if (batch_idx + 1) % config.training.log_interval == 0:
                logger.info(
                    f"Epoch {epoch} | Batch {batch_idx+1}/{len(dl)} | "
                    f"Loss: {loss.item():.4f} (Q: {lq.item():.4f}, C: {lc.item():.4f})"
                )
        
        except RuntimeError as e:
            logger.error(f"è®­ç»ƒé”™è¯¯: {e}")
            if "out of memory" in str(e):
                logger.error("æ˜¾å­˜ä¸è¶³ï¼å»ºè®®å‡å° batch_size")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            raise e
    
    n = len(dl)
    return [t / n for t in totals]


@torch.no_grad()
def evaluate(model, dl, config, logger):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    preds_q, tgts_q, preds_c, tgts_c = [], [], [], []
    
    try:
        for px, ids, mask, q_t, c_t, _, _ in tqdm(dl, desc="è¯„ä¼°ä¸­"):
            px = px.to(config.training.device)
            ids = ids.to(config.training.device)
            mask = mask.to(config.training.device)
            
            q_p, c_p, _, _, _ = model(px, ids, mask)
            
            preds_q.extend(q_p.cpu().numpy().flatten())
            tgts_q.extend(q_t.numpy().flatten())
            preds_c.extend(c_p.cpu().numpy().flatten())
            tgts_c.extend(c_t.numpy().flatten())
        
        # è®¡ç®—æŒ‡æ ‡
        s_q = spearmanr(tgts_q, preds_q).correlation
        s_c = spearmanr(tgts_c, preds_c).correlation
        p_q = pearsonr(tgts_q, preds_q)[0]
        p_c = pearsonr(tgts_c, preds_c)[0]
        
        return s_q, p_q, s_c, p_c
    
    except Exception as e:
        logger.error(f"è¯„ä¼°é”™è¯¯: {e}")
        raise e


def save_checkpoint(model, config, metrics, epoch, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    os.makedirs(config.training.save_dir, exist_ok=True)
    
    # æ„å»ºæ¨¡å‹æ–‡ä»¶å
    if config.model.use_residual_learning:
        if config.model.partial_freeze:
            base_name = f"{config.experiment_name}_residual_partial_{config.model.freeze_layers}L"
        else:
            base_name = f"{config.experiment_name}_residual"
    elif config.model.use_refinement:
        base_name = f"{config.experiment_name}_refinement"
    else:
        base_name = f"{config.experiment_name}"
    
    # ä¿å­˜æœ€æ–°æ¨¡å‹
    checkpoint_path = os.path.join(
        config.training.save_dir,
        f"{base_name}_epoch{epoch}.pt"
    )
    
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'metrics': metrics,
        'config': config
    }, checkpoint_path)
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
    if is_best:
        best_path = os.path.join(
            config.training.save_dir,
            f"{base_name}_best.pt"
        )
        torch.save(model.state_dict(), best_path)
        return best_path
    
    return checkpoint_path


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_parser()
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = parse_args_to_config(args)
    
    # å¦‚æœæŒ‡å®šäº†ä¿å­˜é…ç½®ï¼Œä¿å­˜åé€€å‡º
    if args.save_config:
        config.save(args.save_config)
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {args.save_config}")
        return
    
    # éªŒè¯é…ç½®
    try:
        config.validate()
    except Exception as e:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
        sys.exit(1)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(config)
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ - AIGC å›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹")
    logger.info("=" * 80)
    
    # æ‰“å°é…ç½®
    config.print_summary()
    
    # å‡†å¤‡æ•°æ®
    logger.info("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    try:
        df = pd.read_csv(config.data.data_csv_path)
        from sklearn.model_selection import train_test_split
        train_df, val_df = train_test_split(
            df,
            test_size=config.data.test_size,
            random_state=config.data.random_seed
        )
        
        logger.info(f"  è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
        logger.info(f"  éªŒè¯é›†: {len(val_df)} æ ·æœ¬")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    logger.info("ğŸ”§ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
    proc = CLIPProcessor.from_pretrained(config.data.clip_model_name)
    
    tf_train = transforms.Compose([
        transforms.RandomResizedCrop(config.data.image_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(
            proc.image_processor.image_mean,
            proc.image_processor.image_std
        )
    ])
    
    tf_val = transforms.Compose([
        transforms.Resize((config.data.image_size, config.data.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(
            proc.image_processor.image_mean,
            proc.image_processor.image_std
        )
    ])
    
    train_ds = BaselineDataset(train_df, config.data.image_base_dir, proc, tf_train)
    val_ds = BaselineDataset(val_df, config.data.image_base_dir, proc, tf_val)
    
    train_dl = DataLoader(
        train_ds,
        batch_size=config.training.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=config.training.num_workers,
        pin_memory=True if config.training.device == "cuda" else False
    )
    
    val_dl = DataLoader(
        val_ds,
        batch_size=config.training.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=config.training.num_workers,
        pin_memory=True if config.training.device == "cuda" else False
    )
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
    refinement_cfg = {
        'hidden_dim': config.model.refinement_dim,
        'num_layers': config.model.refinement_layers,
        'num_heads': config.model.refinement_heads
    } if config.model.use_refinement else None
    
    model = BaselineCLIPScore(
        config.data.clip_model_name,
        freeze=config.model.freeze_clip,
        use_refinement=config.model.use_refinement,
        refinement_cfg=refinement_cfg,
        use_two_branch=config.model.use_two_branch,
        use_residual_learning=config.model.use_residual_learning,
        residual_scale_q=config.model.residual_scale_q,
        residual_scale_c=config.model.residual_scale_c,
        partial_freeze=config.model.partial_freeze,
        freeze_layers=config.model.freeze_layers,
        dropout=config.model.dropout
    ).to(config.training.device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    logger.info("âš™ï¸  é…ç½®ä¼˜åŒ–å™¨...")
    opt = torch.optim.AdamW(
        model.parameters(),
        lr=config.training.lr,
        weight_decay=config.training.weight_decay
    )
    
    total_steps = len(train_dl) * config.training.epochs
    warmup_steps = int(config.training.warmup_ratio * total_steps)
    sched = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)
    
    logger.info(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info(f"  Warmup æ­¥æ•°: {warmup_steps}")
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if config.training.mixed_precision else None
    if config.training.mixed_precision:
        logger.info("  âš¡ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
    
    # è®­ç»ƒå¾ªç¯
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    logger.info("=" * 80 + "\n")
    
    best_sc = -1
    best_epoch = 0
    
    try:
        for epoch in range(1, config.training.epochs + 1):
            # è®­ç»ƒ
            train_loss, lq, lc, le = train_epoch(
                model, train_dl, opt, sched, config, scaler, logger, epoch
            )
            
            # è¯„ä¼°
            s_q, p_q, s_c, p_c = evaluate(model, val_dl, config, logger)
            
            # è®°å½•æ—¥å¿—
            log_msg = (
                f"Epoch {epoch}/{config.training.epochs} | "
                f"Train Loss: {train_loss:.4f} (Q: {lq:.4f}, C: {lc:.4f}) | "
                f"Val SROCC: Q={s_q:.4f}, C={s_c:.4f} | "
                f"Val PLCC: Q={p_q:.4f}, C={p_c:.4f}"
            )
            logger.info(log_msg)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            metrics = {
                'train_loss': train_loss,
                'val_srocc_q': s_q,
                'val_srocc_c': s_c,
                'val_plcc_q': p_q,
                'val_plcc_c': p_c
            }
            
            is_best = s_c > best_sc
            if is_best:
                best_sc = s_c
                best_epoch = epoch
            
            if config.training.save_best_only:
                if is_best:
                    save_path = save_checkpoint(model, config, metrics, epoch, is_best=True)
                    logger.info(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹! SROCC_C={s_c:.4f} -> {save_path}")
            else:
                save_path = save_checkpoint(model, config, metrics, epoch, is_best=is_best)
                if is_best:
                    logger.info(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹! SROCC_C={s_c:.4f}")
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        raise e
    
    # è®­ç»ƒå®Œæˆ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"  æœ€ä½³ SROCC_C: {best_sc:.4f} (Epoch {best_epoch})")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
